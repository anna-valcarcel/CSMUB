{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib.metadata import version\n",
    "# version('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VmDgKifbltJP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 575 stations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_711583/361078536.py:497: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stations_df['shapefile_code'].iloc[s] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "finished:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_711583/361078536.py:553: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stations_df['NaNs'].iloc[s] = len(invalid_indices)\n",
      "/tmp/ipykernel_711583/361078536.py:573: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stations_df['r2'].iloc[s] = r2\n"
     ]
    }
   ],
   "source": [
    "# CYGNSS Streamflow Modeling for Ungauged Basins (CSMUB)\n",
    "\n",
    "# This code combines the delineation of watersheds and the analysis of the CYGNSS watermask files to determine the surface water extent of each basin on a monthly time scale using the functions `crop_dem`, `process_basin` and `waterpx_count`. In addition, the function `read_gauge` loads the empirical streamflow data from text files downloaded from the Global Runoff Data Centre (GRDC) [link text](https://portal.grdc.bafg.de/applications/public.html?publicuser=PublicUser#dataDownload/Home). Lastly, it outputs a .csv file with the station name and the results of the analysis.\n",
    "#\n",
    "# ---\n",
    "#\n",
    "# Author(s): Anna Valcarcel\n",
    "#\n",
    "# Last Updated: November 19, 2024\n",
    "\n",
    "#########################################################################\n",
    "# 0.0 - IMPORT PACKAGES #\n",
    "#########################################################################\n",
    "\n",
    "# get_ipython().system('python3.6 -m pip install pysheds netCDF4 fiona geopandas xarray pyshp')\n",
    "# python3.6 -m pip install pysheds netCDF4 fiona geopandas xarray pyshp\n",
    "\n",
    "# mkdir /global/scratch/users/arvalcarcel/tmp\n",
    "# export TMPDIR=/global/scratch/users/arvalcarcel/tmp # set TMPDIR\n",
    "\n",
    "import pysheds\n",
    "from pysheds.grid import Grid\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fiona\n",
    "import shapefile\n",
    "import math\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "from shapely.geometry import Point, shape, box\n",
    "from shapely.vectorized import contains\n",
    "from shapely.strtree import STRtree\n",
    "import matplotlib.path as mpath\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from rasterio.coords import BoundingBox\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#########################################################################\n",
    "# 1.0 DEFINE FUNCTIONS #\n",
    "#########################################################################\n",
    "\n",
    "def crop_dem(input_tif, output_tif, center_lon, center_lat,area):\n",
    "\n",
    "    # # Define the path to the input and output TIFF files\n",
    "\n",
    "    if area < 10000:\n",
    "      width = np.sqrt(area)*4 # width extending from center point to each edge of square\n",
    "    else:\n",
    "      width = np.sqrt(area)*2\n",
    "\n",
    "    width = int(width)\n",
    "    degrees = np.round(width/111.32,2)\n",
    "\n",
    "    xmin = center_lon - degrees\n",
    "    xmax = center_lon + degrees\n",
    "    ymin = center_lat - degrees\n",
    "    ymax = center_lat + degrees\n",
    "\n",
    "    # Define the bounding box coordinates (left, bottom, right, top)\n",
    "    bbox = (xmin, ymin, xmax, ymax)  # Replace with actual coordinates\n",
    "\n",
    "    # Open the source TIFF file\n",
    "    with rasterio.open(input_tif) as src:\n",
    "        # Convert the bounding box to a GeoJSON-style geometry for rasterio.mask.mask\n",
    "        bbox_geom = {\n",
    "            \"type\": \"Polygon\",\n",
    "            \"coordinates\": [[\n",
    "                [bbox[0], bbox[1]],\n",
    "                [bbox[0], bbox[3]],\n",
    "                [bbox[2], bbox[3]],\n",
    "                [bbox[2], bbox[1]],\n",
    "                [bbox[0], bbox[1]]\n",
    "            ]]\n",
    "        }\n",
    "\n",
    "        # Crop the image using the bounding box geometry\n",
    "        out_image, out_transform = mask(src, [bbox_geom], crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "\n",
    "        # Update metadata to reflect the new cropped area\n",
    "        out_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": out_image.shape[1],\n",
    "            \"width\": out_image.shape[2],\n",
    "            \"transform\": out_transform,\n",
    "            'crs': src.crs  # Ensure the CRS remains the same\n",
    "        })\n",
    "\n",
    "        # # Save the cropped data to a new GeoTIFF file with updated profile\n",
    "        # profile = src.profile\n",
    "        # profile.update({\n",
    "        #     'height': height,\n",
    "        #     'width': width,\n",
    "        #     'transform': transform,\n",
    "        #     'crs': src.crs  # Ensure CRS is preserved\n",
    "        # })\n",
    "\n",
    "        with rasterio.open(output_tif, 'w', **out_meta) as dst:\n",
    "            dst.write(out_image)\n",
    "\n",
    "\n",
    "    # Calculate the extent of the image in geographic coordinates\n",
    "    left = out_transform.c  # xmin\n",
    "    right = out_transform.c + out_transform.a * out_image.shape[2]  # xmax\n",
    "    top = out_transform.f  # ymax\n",
    "    bottom = out_transform.f + out_transform.e * out_image.shape[1]  # ymin\n",
    "\n",
    "    extent = (left, right, bottom, top)\n",
    "\n",
    "\n",
    "\n",
    "def process_basin(tif_input, pour_lon, pour_lat,shp_name,number):\n",
    "# ----------------------------\n",
    "    # LOAD THE TIF FILE\n",
    "    dset = rasterio.open(tif_input,mode='r+')\n",
    "    # dset.nodata = -32767\n",
    "\n",
    "    # OPTIONAL: change elevation values of ocean\n",
    "    # rdbl = dset.read(1)\n",
    "    # for i in range(0,len(rdbl)):\n",
    "    #     list1 = rdbl[i]\n",
    "    #     for j in range(0,len(list1)):\n",
    "    #         if list1[j] == 32767:\n",
    "    #             list1[j] = -1\n",
    "    #     rdbl[i] = list1\n",
    "\n",
    "    dset.close()\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    # Read elevation raster\n",
    "    grid = Grid.from_raster(tif_input)\n",
    "    dem = grid.read_raster(tif_input)\n",
    "    # # Open and process the raster\n",
    "    # with rasterio.open(tif_input, \"r\") as src:\n",
    "    #     profile = src.profile\n",
    "    #     data = src.read(1)\n",
    "\n",
    "    #     # Replace NoData values\n",
    "    #     data[data == 32767.0] = -99.0\n",
    "\n",
    "    #     # Update the profile to reflect the new NoData value\n",
    "    #     profile.update(dtype=\"float32\", nodata=-99.0)\n",
    "\n",
    "    # new_tif = f'/global/scratch/users/arvalcarcel/CSMUB/DATA/DEM/STATIONS/{number}/{number}_fixed.tif'\n",
    "    # # Write the updated raster\n",
    "    # with rasterio.open(new_tif, \"w\", **profile) as dst:\n",
    "    #     dst.write(data, 1)\n",
    "\n",
    "    # # Debug to confirm changes\n",
    "    # with rasterio.open(new_tif) as src:\n",
    "    #     print(\"Updated Data type:\", src.dtypes[0])\n",
    "    #     print(\"Updated NoData value:\", src.nodata)\n",
    "    #     print(\"Updated Min/Max values:\", data.min(), data.max())\n",
    "    #     print(\"Number of NoData cells:\", np.sum(data == -99.0))\n",
    "\n",
    "    # # Step 1: Initialize the grid and read raster\n",
    "    # grid = Grid.from_raster(new_tif, data_name=\"dem\", nodata=np.float32(-99.0))\n",
    "\n",
    "    # # Step 2: Explicitly read raster and set nodata value\n",
    "    # dem = grid.read_raster(new_tif, nodata=np.float32(-99.0))\n",
    "\n",
    "    # Condition DEM\n",
    "    # ----------------------\n",
    "    # Fill pits in DEM\n",
    "    pit_filled_dem = grid.fill_pits(dem)\n",
    "\n",
    "    # Fill depressions in DEM\n",
    "    flooded_dem = grid.fill_depressions(pit_filled_dem)\n",
    "\n",
    "    # Resolve flats in DEM\n",
    "    inflated_dem = grid.resolve_flats(flooded_dem)\n",
    "    print(inflated_dem.dtype)\n",
    "\n",
    "    # Determine D8 flow directions from DEM\n",
    "    # ----------------------\n",
    "    # Specify directional mapping\n",
    "    dirmap = (64, 128, 1, 2, 4, 8, 16, 32)\n",
    "\n",
    "    # Compute flow directions\n",
    "    # -------------------------------------\n",
    "    fdir = grid.flowdir(inflated_dem, dirmap=dirmap)\n",
    "\n",
    "    # Calculate flow accumulation\n",
    "    # --------------------------\n",
    "    acc = grid.accumulation(fdir, dirmap=dirmap)\n",
    "    acc[dem==0] = 0\n",
    "\n",
    "    # Delineate a catchment\n",
    "    # ---------------------\n",
    "    # Specify pour point\n",
    "    x = pour_lon\n",
    "    y = pour_lat\n",
    "    # print(x,y)\n",
    "\n",
    "    # # Snap pour point to high accumulation cell\n",
    "    # x_snap, y_snap = grid.snap_to_mask(acc > 1000000, (x, y))\n",
    "    # # print(x_snap,y_snap)\n",
    "\n",
    "    # Snap pour point to high accumulation cell\n",
    "    acc_max = int(acc.max())\n",
    "    power_ten = len(str(acc_max))\n",
    "    snap_pt = 10**(int(power_ten) - 1)\n",
    "    x_snap, y_snap = grid.snap_to_mask(acc > snap_pt, (x, y))\n",
    "\n",
    "    # snap_pt = round(acc_max*0.9,-3)\n",
    "    # x_snap, y_snap = grid.snap_to_mask(acc > snap_pt, (x, y))\n",
    "\n",
    "    # Delineate the catchment\n",
    "    catch = grid.catchment(x=x_snap, y=y_snap, fdir=fdir, dirmap=dirmap,\n",
    "                          xytype='coordinate')\n",
    "\n",
    "\n",
    "    # Crop and plot the catchment\n",
    "    # ---------------------------\n",
    "    # Clip the bounding box to the catchment\n",
    "    grid.clip_to(catch)\n",
    "    catch_view = grid.view(catch, dtype=np.uint8)\n",
    "\n",
    "    # Create a vector representation of the catchment mask\n",
    "    shapes = grid.polygonize(catch_view)\n",
    "\n",
    "    # Specify schema\n",
    "    schema = {\n",
    "            'geometry': 'Polygon',\n",
    "            'properties': {'LABEL': 'float:16'}\n",
    "    }\n",
    "\n",
    "    # Write shapefile\n",
    "    shp_output = shp_name\n",
    "    with fiona.open(shp_output, 'w',\n",
    "                    driver='ESRI Shapefile',\n",
    "                    crs=grid.crs.srs,\n",
    "                    schema=schema) as c:\n",
    "        i = 0\n",
    "        for shape, value in shapes:\n",
    "            rec = {}\n",
    "            rec['geometry'] = shape\n",
    "            rec['properties'] = {'LABEL' : str(value)}\n",
    "            rec['id'] = str(i)\n",
    "            c.write(rec)\n",
    "            i += 1\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(8,6))\n",
    "    # fig.patch.set_alpha(0)\n",
    "    # plt.grid('on', zorder=0)\n",
    "    # im = ax.imshow(acc, extent=catch.extent, zorder=1,\n",
    "    #               cmap='ocean',\n",
    "    #               norm=colors.LogNorm(1, acc.max()),\n",
    "    #               interpolation='bilinear')\n",
    "    # plt.colorbar(im, ax=ax, label='Upstream Cells')\n",
    "    # plt.imshow(np.where(catch_view,catch_view, np.nan), extent=grid.extent,zorder=2, cmap='Greys',alpha=0.3)\n",
    "    # plt.scatter(x_snap,y_snap, color='red', s=100, marker='o', zorder=3)\n",
    "    # plt.xlabel('Longitude')\n",
    "    # plt.ylabel('Latitude')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    return shp_output\n",
    "\n",
    "\n",
    "\n",
    "def get_stationdata(station_file):\n",
    "  # Remove spaces in column names entirely\n",
    "  df_Q = pd.read_csv(station_file, delimiter=';',encoding='utf-8',skiprows=38)\n",
    "  # print(df_Q.head)\n",
    "\n",
    "  df_Q.columns = df_Q.columns.str.replace(' ', '')\n",
    "  df_Q['YYYY-MM-DD'] = df_Q['YYYY-MM-DD'].str.slice(0, 7)\n",
    "\n",
    "  # Convert 'YYYY-MM-DD' column to datetime format\n",
    "  df_Q['YYYY-MM-DD'] = pd.to_datetime(df_Q['YYYY-MM-DD'], format='%Y-%m')\n",
    "\n",
    "  # Convert cutoff_date to datetime format\n",
    "  start_date = pd.to_datetime('2018-08', format='%Y-%m')\n",
    "  end_date = pd.to_datetime('2024-09', format='%Y-%m')\n",
    "\n",
    "  # Filter the DataFrame to keep rows after '2018-08'\n",
    "  df_filtered = df_Q[df_Q['YYYY-MM-DD'] >= start_date]\n",
    "  df_filtered = df_filtered[df_filtered['YYYY-MM-DD'] <= end_date]\n",
    "\n",
    "  stream_gauge = df_filtered['Calculated'].values\n",
    "\n",
    "  df_filtered['YYYY-MM-DD'] = df_filtered['YYYY-MM-DD'].dt.strftime('%Y-%m')\n",
    "  dates = df_filtered['YYYY-MM-DD']\n",
    "  dates = dates.reset_index()\n",
    "\n",
    "  return stream_gauge, dates\n",
    "\n",
    "\n",
    "\n",
    "def waterpx_count(shp_input, nc_input):\n",
    "\n",
    "  # Open NetCDF file and extract variables\n",
    "  with nc.Dataset(nc_input) as dataset:\n",
    "      watermask = dataset.variables['watermask'][:]\n",
    "      latitude = dataset.variables['lat'][:]\n",
    "      longitude = dataset.variables['lon'][:]\n",
    "\n",
    "  # Load and reproject the shapefile\n",
    "  shp = gpd.read_file(shp_input).to_crs('EPSG:4326')\n",
    "  minlon, minlat, maxlon, maxlat = shp.geometry.total_bounds\n",
    "\n",
    "  # Limit the NetCDF data to the bounding box of the shapefile\n",
    "  lat_mask = (latitude >= minlat) & (latitude <= maxlat)\n",
    "  lon_mask = (longitude >= minlon) & (longitude <= maxlon)\n",
    "\n",
    "  watermask = watermask[lat_mask, :][:, lon_mask]\n",
    "  lat_filtered = latitude[lat_mask]\n",
    "  lon_filtered = longitude[lon_mask]\n",
    "\n",
    "  # Step 1: Create a grid of filtered points\n",
    "  lon_grid, lat_grid = np.meshgrid(lon_filtered, lat_filtered)\n",
    "  points = np.column_stack([lon_grid.ravel(), lat_grid.ravel()])\n",
    "\n",
    "  # Flatten the watermask array to align with points\n",
    "  watermask_flat = watermask.ravel()\n",
    "\n",
    "  # Step 2: Load the shapefile and get combined geometry\n",
    "  shapefile_geom = shp.geometry.unary_union  # Combine all geometries in the shapefile\n",
    "\n",
    "  # Step 3: Identify points intersecting the shapefile\n",
    "  intersects_mask = contains(shapefile_geom, points[:, 0], points[:, 1])\n",
    "\n",
    "  # Step 4: Filter the points and watermask values\n",
    "  filtered_points = points[intersects_mask]\n",
    "  filtered_watermask = watermask_flat[intersects_mask]\n",
    "\n",
    "  # Step 5: Create geometries for intersecting points\n",
    "  geometries = [Point(lon, lat) for lon, lat in filtered_points]\n",
    "\n",
    "  # Step 6: Create a GeoDataFrame\n",
    "  gdf = gpd.GeoDataFrame({'watermask': filtered_watermask}, geometry=geometries, crs='EPSG:4326')\n",
    "\n",
    "\n",
    "  markercolormap2= colors.ListedColormap(['white', 'black','blue'])\n",
    "\n",
    "  # Step 1: Filter the GeoDataFrame where 'watermask' is equal to 2\n",
    "  filtered_gdf = gdf[gdf['watermask'] == 1]\n",
    "\n",
    "  water_pixels = len(filtered_gdf)\n",
    "  total_pixels = len(gdf)\n",
    "\n",
    "  water_percent = (water_pixels / total_pixels) * 100\n",
    "\n",
    "  # if stream == -999 or stream < -100 :\n",
    "  #     water_percent = np.nan\n",
    "  #     stream = np.nan\n",
    "\n",
    "\n",
    "  return water_pixels, water_percent, total_pixels\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# 2.0 READ MASTERFILE #\n",
    "#########################################################################\n",
    "\n",
    "masterlist = '/global/scratch/users/arvalcarcel/CSMUB/DATA/GRDC_Stations_AllMonthly.csv'\n",
    "\n",
    "stations_df = pd.read_csv(masterlist)\n",
    "station_num = stations_df['grdc_no']\n",
    "\n",
    "monthly_path = '/global/scratch/users/arvalcarcel/CSMUB/DATA/STATIONS/'\n",
    "\n",
    "station_files = [os.path.join(monthly_path, f\"{station_no}_Q_Month.txt\") for station_no in station_num]\n",
    "\n",
    "stations_df['shapefile_code'] = np.ones(len(station_files)) * 999\n",
    "stations_df['r2'] = np.ones(len(station_files)) * 999\n",
    "stations_df['NaNs'] = np.ones(len(station_files)) * 999\n",
    "\n",
    "print(f\"Loaded {len(station_files)} stations.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################# START OF LOOP #############################\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "# 2.1 CROP CONTINENT SCALE DEM #\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "for s in range(0,1):\n",
    "# for s in range(44,45):\n",
    "    data = stations_df.iloc[s]\n",
    "    \n",
    "    number = data['grdc_no']\n",
    "    region = data['wmo_reg']\n",
    "    river = data['river']\n",
    "    country = data['country']\n",
    "    name = data['station']\n",
    "    lat = data['lat']\n",
    "    lon = data['long']\n",
    "    area = data['area']\n",
    "    altitude = data['altitude']\n",
    "    \n",
    "    \n",
    "    # determine what continent DEM to use\n",
    "    # 1 - africa, 2 - asia, 3 - SA, 4 - NA/CA/caribbean, 5 - SW pacific, 6 - europe\n",
    "    if region == 1:\n",
    "        dem = '/global/scratch/users/arvalcarcel/CSMUB/DATA/DEM/hyd_af_dem_30s.tif'\n",
    "    elif region == 2:\n",
    "        dem = '/global/scratch/users/arvalcarcel/CSMUB/DATA/DEM/hyd_as_dem_30s.tif'\n",
    "    elif region == 3:\n",
    "        dem = '/global/scratch/users/arvalcarcel/CSMUB/DATA/DEM/hyd_sa_dem_30s.tif'\n",
    "    elif region == 4:\n",
    "        dem = '/global/scratch/users/arvalcarcel/CSMUB/DATA/DEM/hyd_na_dem_30s.tif'\n",
    "    elif region == 5:\n",
    "        dem = '/global/scratch/users/arvalcarcel/CSMUB/DATA/DEM/hyd_au_dem_30s.tif'\n",
    "    elif region == 6:\n",
    "        dem = '/global/scratch/users/arvalcarcel/CSMUB/DATA/DEM/hyd_eu_dem_30s.tif'\n",
    "\n",
    "    # Define the path for the new folder\n",
    "    new_folder_path = f'/global/home/users/arvalcarcel/ondemand/data/dem/{number}/'\n",
    "    \n",
    "    # Create the folder\n",
    "    # os.makedirs(new_folder_path, exist_ok=True)\n",
    "    \n",
    "    output_tif = f'/global/home/users/arvalcarcel/ondemand/data/dem/{number}/{number}_dem.tif' # save the cropped tif file\n",
    "    \n",
    "    # crop_dem(dem, output_tif, lon, lat,area)\n",
    "    \n",
    "    \n",
    "    #########################################################################\n",
    "    # 2.2 DELINEATE BASIN #\n",
    "    #########################################################################\n",
    "    \n",
    "    \n",
    "    # LOAD THE TIF FILE\n",
    "    tif_input = output_tif # open the croppe tif file and delineate basin from it\n",
    "    pour_point = [lon,lat]\n",
    "    \n",
    "    # Define the path for the new folder\n",
    "    new_folder_path = f'/global/home/users/arvalcarcel/ondemand/data/dem/{number}/'\n",
    "    \n",
    "    # Create the folder\n",
    "    # os.makedirs(new_folder_path, exist_ok=True)\n",
    "    \n",
    "    # Create new shapefile\n",
    "    shp_name = new_folder_path + f'{number}.shp'\n",
    "    shp_output = shp_name\n",
    "    # shp_output = process_basin(tif_input, lon, lat,shp_name,number)\n",
    "    \n",
    "    #########################################################################\n",
    "    # 2.3 DETERMINE SHAPEFILE #\n",
    "    #########################################################################\n",
    "    \n",
    "    \n",
    "    # Read the shapefiles\n",
    "    shapefile1 = gpd.read_file(shp_output) # delineated shapefile\n",
    "    shapefile2 = gpd.read_file(f'/global/scratch/users/arvalcarcel/CSMUB/DATA/SHAPEFILES/{number}/{number}.shp') # GRDC shapefile\n",
    "    \n",
    "    # Step 3: Reproject to a projected CRS (e.g., UTM Zone 33N, replace EPSG:32633 as appropriate)\n",
    "    shapefile1 = shapefile1.to_crs(epsg=32633)\n",
    "    shapefile2 = shapefile2.to_crs(epsg=32633)\n",
    "    \n",
    "    # Ensure CRS consistency\n",
    "    assert shapefile1.crs == shapefile2.crs, \"CRS mismatch between shapefiles!\"\n",
    "    \n",
    "    # Validate geometries\n",
    "    shapefile1['geometry'] = shapefile1.geometry.buffer(0)\n",
    "    shapefile2['geometry'] = shapefile2.geometry.buffer(0)\n",
    "    \n",
    "    # Filter geometries that potentially intersect\n",
    "    possible_intersections = shapefile1[shapefile1.geometry.apply(lambda g: shapefile2.geometry.intersects(g).any())]\n",
    "    \n",
    "    # Perform the intersection\n",
    "    intersection = gpd.overlay(possible_intersections, shapefile2, how='intersection',keep_geom_type=False)\n",
    "    \n",
    "    # Calculate area\n",
    "    intersection_area = intersection.geometry.area.sum() / 10**6  # Convert m² to km²\n",
    "\n",
    "    inter_pcnt = (intersection_area/area)*100\n",
    "    \n",
    "    # print('area: ', area)\n",
    "    # print('intersection area: ', intersection_area)\n",
    "    # print('percent: ', inter_pcnt)\n",
    "    \n",
    "    if 200 > inter_pcnt >= 50:\n",
    "        shapefile = shp_output\n",
    "        stations_df['shapefile_code'].iloc[s] = 1\n",
    "    else:\n",
    "        shapefile = f'/global/scratch/users/arvalcarcel/CSMUB/DATA/SHAPEFILES/{number}/{number}.shp'\n",
    "        stations_df['shapefile_code'].iloc[s] = 2\n",
    "    \n",
    "    # print(shp_log[s])\n",
    "    \n",
    "    #########################################################################\n",
    "    # 2.3 IMPORT STATION DATA #\n",
    "    #########################################################################\n",
    "    \n",
    "    \n",
    "    q_file = f'/global/scratch/users/arvalcarcel/CSMUB/DATA/STATIONS/{number}_Q_Month.txt'\n",
    "    stream_gauge, dates = get_stationdata(q_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #########################################################################\n",
    "    # 2.4 CALCULATE WATER PIXEL PERCENT #\n",
    "    #########################################################################\n",
    "    \n",
    "    \n",
    "    directory = \"/global/scratch/users/arvalcarcel/CSMUB/DATA/CYGNSS/\"\n",
    "    file_list = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    \n",
    "    \n",
    "    # Create empty numpy arrays with the correct size\n",
    "    ncdf_list = []\n",
    "    water_px = np.zeros(len(dates))\n",
    "    water_pcnt = np.zeros(len(dates)) # CHANGE TO DATES\n",
    "    tot_px = np.zeros(len(dates))\n",
    "    \n",
    "    \n",
    "    for i in range(0,len(dates)):\n",
    "        date = dates['YYYY-MM-DD'][i]\n",
    "        ncdf_name = f'/global/scratch/users/arvalcarcel/CSMUB/DATA/CYGNSS/cyg.ddmi.{date}.l3.uc-berkeley-watermask-monthly.a31.d32.nc'\n",
    "        ncdf_list.append(ncdf_name)\n",
    "    \n",
    "    \n",
    "    for f, filename in enumerate(ncdf_list):\n",
    "        if os.path.isfile(filename):\n",
    "            pixel_count, pixel_percent, total_pixels = waterpx_count(shp_output, filename)\n",
    "            water_px[f] = pixel_count\n",
    "            water_pcnt[f] = pixel_percent\n",
    "            tot_px[f] = total_pixels\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #########################################################################\n",
    "    # 2.5 EXPORT AND SAVE RESULTS #\n",
    "    #########################################################################\n",
    "    \n",
    "\n",
    "    # Identify indices where stream_gauge has the value -999\n",
    "    invalid_indices = [i for i, value in enumerate(stream_gauge) if value == -999]\n",
    "    stations_df['NaNs'].iloc[s] = len(invalid_indices)\n",
    "    # print('invalid indices : ',invalid_indices, ' of ', len(stream_gauge))\n",
    "    # print('total nans: ', nan_no[s])\n",
    "\n",
    "    # Remove items at those indices from all three lists\n",
    "    stream_gauge = [value for i, value in enumerate(stream_gauge) if i not in invalid_indices]\n",
    "    dates = [value for i, value in enumerate(dates['YYYY-MM-DD']) if i not in invalid_indices]\n",
    "    water_pcnt = [value for i, value in enumerate(water_pcnt) if i not in invalid_indices]\n",
    "    water_area = [x * area for x in water_pcnt]\n",
    "\n",
    "    df_final = pd.DataFrame({'Date': dates, 'Q': stream_gauge, 'SWE': water_pcnt, 'SWE_scaled': water_area})\n",
    "    \n",
    "    df_final.to_csv(f\"/global/home/users/arvalcarcel/ondemand/results/csv/{number}.csv\")\n",
    "    \n",
    "    def check(list):\n",
    "        return all(i == list[0] for i in list)\n",
    "\n",
    "    if check(stream_gauge) == False:\n",
    "        slope, intercept, r, p, se = stats.linregress(stream_gauge, water_pcnt)\n",
    "        r2 = r**2\n",
    "        stations_df['r2'].iloc[s] = r2\n",
    "    \n",
    "    \n",
    "        # fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "        # ax1.plot(df_final['Date'],df_final['Q'], color='b', label='Streamflow')\n",
    "        # ax1.set_xlabel('Date')\n",
    "        # ax1.set_ylabel('Streamflow [m^3/s]', color='b')\n",
    "        # ax1.set_title(f'{river} ({number}), {country}: Streamflow vs Watermask Timeseries')\n",
    "        # ax1.set_xticks(df_final['Date'])\n",
    "        # ax1.set_xticklabels(df_final['Date'])\n",
    "        # plt.xticks(rotation=60)\n",
    "        \n",
    "        # ax2 = ax1.twinx()\n",
    "        # ax2.plot(df_final['Date'],df_final['SWE'], color='r', label='Watermask')\n",
    "        # ax2.set_ylabel('Watermask Px Percent', color='r')\n",
    "        # # major ticks every 10\n",
    "        # ax1.xaxis.set_major_locator(MultipleLocator(3))\n",
    "        \n",
    "        # # minor ticks at every point\n",
    "        # ax1.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "        # # Adding the legend inside the plot\n",
    "        # lines, labels = ax1.get_legend_handles_labels()\n",
    "        # lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        # ax1.legend(lines + lines2, labels + labels2, loc='best', ncol=1)\n",
    "        # plt.savefig(f\"/global/home/users/arvalcarcel/ondemand/results/timeseries/{number}.png\",bbox_inches = \"tight\")\n",
    "        # plt.show()\n",
    "\n",
    "    else:\n",
    "        stations_df['r2'].iloc[s] = np.nan\n",
    "        stations_df['NaNs'].iloc[s] = np.nan\n",
    "    \n",
    "\n",
    "    # stations_df['shapefile_code'][s] = shp_log[s]\n",
    "    # stations_df['r2'][s] = r_squared[s]\n",
    "    # stations_df['NaNs'][s] = nan_no[s]\n",
    "\n",
    "    stations_df.to_csv(f\"/global/home/users/arvalcarcel/ondemand/results/ALL_STATIONS_FINAL.csv\")\n",
    "    print(s)\n",
    "    ############################## END OF LOOP ##############################\n",
    "\n",
    "\n",
    "\n",
    "print('finished: ', s)\n",
    "\n",
    "#########################################################################\n",
    "# END OF CODE #\n",
    "#########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(water_pcnt.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify indices where stream_gauge has the value -999\n",
    "# invalid_indices = [i for i, value in enumerate(stream_gauge) if value == -999]\n",
    "# nan_no[s] = len(invalid_indices)\n",
    "# print('invalid indices : ',invalid_indices, ' of ', len(stream_gauge))\n",
    "# print('total nans: ', nan_no[s])\n",
    "\n",
    "# # Remove items at those indices from all three lists\n",
    "# stream_gauge = [value for i, value in enumerate(stream_gauge) if i not in invalid_indices]\n",
    "# print(len(stream_gauge))\n",
    "# dates['YYYY-MM-DD'] = [value for i, value in enumerate(dates['YYYY-MM-DD']) if i not in invalid_indices]\n",
    "# print(len(dates))\n",
    "# water_pcnt = [value for i, value in enumerate(water_pcnt) if i not in invalid_indices]\n",
    "# print(len(water_pcnt))\n",
    "\n",
    "\n",
    "# df_final = pd.DataFrame({'Date': dates['YYYY-MM-DD'], 'Q': stream_gauge, 'SWE': water_pcnt})\n",
    "\n",
    "# df_final.to_csv(f\"/global/scratch/users/arvalcarcel/CSMUB/RESULTS/CSV/{number}.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
