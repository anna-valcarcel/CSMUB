{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c0a9c4-8bd7-423f-9a98-5a1106efd184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 18:09:52.860305: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-06 18:09:52.862497: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-06 18:09:52.867755: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-06 18:09:52.883080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746580192.908682 2430280 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746580192.916133 2430280 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-06 18:09:52.942296: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import SimpleRNN , Input,Flatten, LSTM\n",
    "from tensorflow.keras import backend as K\n",
    "from keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a71e2c-2f9c-4c14-a4eb-09a584f9a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0     Date       Q       SWE    SWE_scaled  GRDC_No      Area  \\\n",
      "0               0  2018-08  24.526  2.087791  1.809042e+06  1159100  866486.0   \n",
      "1               1  2018-09  31.372  1.835435  1.590379e+06  1159100  866486.0   \n",
      "2               2  2018-10  19.572  1.976332  1.712464e+06  1159100  866486.0   \n",
      "3               3  2018-11   7.349  1.633273  1.415208e+06  1159100  866486.0   \n",
      "4               4  2018-12  13.824  1.782850  1.544815e+06  1159100  866486.0   \n",
      "...           ...      ...     ...       ...           ...      ...       ...   \n",
      "23053          57  2023-05   3.046  3.400061  5.610100e+04  6594090   16500.0   \n",
      "23054          58  2023-06   2.489  3.195970  5.273351e+04  6594090   16500.0   \n",
      "23055          59  2023-07   2.021  3.656259  6.032828e+04  6594090   16500.0   \n",
      "23056          60  2023-08   2.067  3.578097  5.903860e+04  6594090   16500.0   \n",
      "23057          61  2023-09   2.483  3.630205  5.989839e+04  6594090   16500.0   \n",
      "\n",
      "       Latitude  Avg Slope  Max Slope     Aridity  \n",
      "0     -28.75799   0.745665  22.007082   907.27856  \n",
      "1     -28.75799   0.745665  22.007082   907.27856  \n",
      "2     -28.75799   0.745665  22.007082   907.27856  \n",
      "3     -28.75799   0.745665  22.007082   907.27856  \n",
      "4     -28.75799   0.745665  22.007082   907.27856  \n",
      "...         ...        ...        ...         ...  \n",
      "23053  31.84000   2.749054  25.081081  1116.96740  \n",
      "23054  31.84000   2.749054  25.081081  1116.96740  \n",
      "23055  31.84000   2.749054  25.081081  1116.96740  \n",
      "23056  31.84000   2.749054  25.081081  1116.96740  \n",
      "23057  31.84000   2.749054  25.081081  1116.96740  \n",
      "\n",
      "[23058 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "opened = []\n",
    "\n",
    "csv_path = '/global/scratch/users/arvalcarcel/CSMUB/RESULTS/CSV/'\n",
    "masterlist = '/global/scratch/users/arvalcarcel/CSMUB/RESULTS/ALL_STATIONS_FINAL_REVISED.csv'\n",
    "\n",
    "# Load the master list of stations\n",
    "stations_df = pd.read_csv(masterlist)\n",
    "\n",
    "# Extract station numbers, areas, and latitudes\n",
    "station_num = stations_df['grdc_no']\n",
    "station_area = stations_df['area']\n",
    "station_lat = stations_df['lat']\n",
    "station_avgslope = stations_df['avg_slope']\n",
    "station_maxslope = stations_df['max_slope']\n",
    "station_aridity = stations_df['avg_aridity']\n",
    "\n",
    "# Map station numbers to areas and latitudes\n",
    "station_area_map = dict(zip(station_num, station_area))\n",
    "station_lat_map = dict(zip(station_num, station_lat))\n",
    "station_avgslope_map = dict(zip(station_num, station_avgslope))\n",
    "station_maxslope_map = dict(zip(station_num, station_maxslope))\n",
    "station_aridity_map = dict(zip(station_num, station_aridity))\n",
    "\n",
    "# Generate the list of file paths\n",
    "arrayFile = [os.path.join(csv_path, f\"{station_no}.csv\") for station_no in station_num]\n",
    "# print(arrayFile)\n",
    "# Initialize a list to store opened DataFrames\n",
    "for file in arrayFile:\n",
    "    station_no = os.path.basename(file).split('.')[0]\n",
    "    # print(station_no)# Extract station number from the filename\n",
    "    if os.path.exists(file):  # Check if file exists\n",
    "        df = pd.read_csv(file, index_col=None, header=0)\n",
    "        station_no_int = int(station_no)  # Convert station number to integer for lookup\n",
    "        df['GRDC_No'] = station_no_int  # Add the station number as a new column\n",
    "        df['Area'] = station_area_map.get(station_no_int, None)  # Add the Area column\n",
    "        df['Latitude'] = station_lat_map.get(station_no_int, None)  # Add the latitude column\n",
    "        df['Avg Slope'] = station_avgslope_map.get(station_no_int, None)\n",
    "        df['Max Slope'] = station_maxslope_map.get(station_no_int, None)\n",
    "        df['Aridity'] = station_aridity_map.get(station_no_int, None)\n",
    "        opened.append(df)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "total_df = pd.concat(opened, axis=0, ignore_index=True)\n",
    "\n",
    "# Print or save the resulting DataFrame\n",
    "print(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2695783-7f50-4cf5-afd5-8e5e25677b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     Date       Q       SWE   SWE_scaled  GRDC_No      Area  \\\n",
      "0           0  2018-08  24.526  2.087791  1809042.021  1159100  866486.0   \n",
      "1           1  2018-09  31.372  1.835435  1590378.590  1159100  866486.0   \n",
      "2           2  2018-10  19.572  1.976332  1712463.856  1159100  866486.0   \n",
      "3           3  2018-11   7.349  1.633273  1415208.035  1159100  866486.0   \n",
      "4           4  2018-12  13.824  1.782850  1544814.625  1159100  866486.0   \n",
      "\n",
      "   Latitude  Avg Slope  Max Slope  ...        11        12        13  \\\n",
      "0 -28.75799   0.745665  22.007082  ...  0.379222  0.354625  0.331737   \n",
      "1 -28.75799   0.745665  22.007082  ...  0.379222  0.354625  0.331737   \n",
      "2 -28.75799   0.745665  22.007082  ...  0.379222  0.354625  0.331737   \n",
      "3 -28.75799   0.745665  22.007082  ...  0.379222  0.354625  0.331737   \n",
      "4 -28.75799   0.745665  22.007082  ...  0.379222  0.354625  0.331737   \n",
      "\n",
      "         14        15      16        17        18   19  20  \n",
      "0  0.327396  0.330816  0.3906  0.421511  0.044986  0.0   0  \n",
      "1  0.327396  0.330816  0.3906  0.421511  0.044986  0.0   0  \n",
      "2  0.327396  0.330816  0.3906  0.421511  0.044986  0.0   0  \n",
      "3  0.327396  0.330816  0.3906  0.421511  0.044986  0.0   0  \n",
      "4  0.327396  0.330816  0.3906  0.421511  0.044986  0.0   0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "landcover = '/global/scratch/users/arvalcarcel/CSMUB/RESULTS/CSV/ALL_STATIONS_LANDCOVER.csv'\n",
    "landcover_df = pd.read_csv(landcover)\n",
    "\n",
    "# Ensure number and GRDC_No are of the same type\n",
    "total_df['GRDC_No'] = total_df['GRDC_No'].astype(int)\n",
    "landcover_df['number'] = landcover_df['number'].astype(int)\n",
    "\n",
    "# Initialize empty lists for new columns\n",
    "new_columns = {col: [] for col in landcover_df.columns if col != 'number'}\n",
    "\n",
    "# Loop through total_df and match with landcover_df\n",
    "for grdc_no in total_df['GRDC_No']:\n",
    "    match = landcover_df[landcover_df['number'] == grdc_no]\n",
    "    \n",
    "    if not match.empty:\n",
    "        for col in new_columns:\n",
    "            new_columns[col].append(match[col].values[0])  # Append matched value\n",
    "    else:\n",
    "        for col in new_columns:\n",
    "            new_columns[col].append(None)  # Append NaN if no match found\n",
    "\n",
    "# Convert lists to a DataFrame and concatenate with total_df\n",
    "new_data = pd.DataFrame(new_columns)\n",
    "total_df = pd.concat([total_df, new_data], axis=1)\n",
    "\n",
    "print(total_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bbe9c49-a524-4151-9260-9daee75f760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     Date       Q       SWE   SWE_scaled  GRDC_No      Area  \\\n",
      "0           0  2018-08  24.526  2.087791  1809042.021  1159100  866486.0   \n",
      "1           1  2018-09  31.372  1.835435  1590378.590  1159100  866486.0   \n",
      "2           2  2018-10  19.572  1.976332  1712463.856  1159100  866486.0   \n",
      "3           3  2018-11   7.349  1.633273  1415208.035  1159100  866486.0   \n",
      "4           4  2018-12  13.824  1.782850  1544814.625  1159100  866486.0   \n",
      "\n",
      "   Latitude  Avg Slope  Max Slope  ...         7    8         9   10   11  \\\n",
      "0 -28.75799   0.745665  22.007082  ...  0.632411  0.0  1.969697  0.0  0.0   \n",
      "1 -28.75799   0.745665  22.007082  ...  0.632411  0.0  1.969697  0.0  0.0   \n",
      "2 -28.75799   0.745665  22.007082  ...  0.632411  0.0  1.969697  0.0  0.0   \n",
      "3 -28.75799   0.745665  22.007082  ...  0.632411  0.0  1.969697  0.0  0.0   \n",
      "4 -28.75799   0.745665  22.007082  ...  0.632411  0.0  1.969697  0.0  0.0   \n",
      "\n",
      "         12   13   14   15   16  \n",
      "0  0.685112  0.0  0.0  0.0  0.0  \n",
      "1  0.685112  0.0  0.0  0.0  0.0  \n",
      "2  0.685112  0.0  0.0  0.0  0.0  \n",
      "3  0.685112  0.0  0.0  0.0  0.0  \n",
      "4  0.685112  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "soiltype = '/global/scratch/users/arvalcarcel/CSMUB/RESULTS/CSV/ALL_STATIONS_SOILTYPE.csv'\n",
    "soiltype_df = pd.read_csv(soiltype)\n",
    "# print(soiltype_df.head())\n",
    "# Ensure number and GRDC_No are of the same type\n",
    "total_df['GRDC_No'] = total_df['GRDC_No'].astype(int)\n",
    "soiltype_df['number'] = soiltype_df['number'].astype(int)\n",
    "\n",
    "# Initialize empty lists for new columns\n",
    "new_columns = {col: [] for col in soiltype_df.columns if col != 'number'}\n",
    "\n",
    "# Loop through total_df and match with landcover_df\n",
    "for grdc_no in total_df['GRDC_No']:\n",
    "    match = soiltype_df[soiltype_df['number'] == grdc_no]\n",
    "    \n",
    "    if not match.empty:\n",
    "        for col in new_columns:\n",
    "            new_columns[col].append(match[col].values[0])  # Append matched value\n",
    "    else:\n",
    "        for col in new_columns:\n",
    "            new_columns[col].append(None)  # Append NaN if no match found\n",
    "\n",
    "# Convert lists to a DataFrame and concatenate with total_df\n",
    "new_data = pd.DataFrame(new_columns)\n",
    "total_df = pd.concat([total_df, new_data], axis=1)\n",
    "\n",
    "print(total_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f26f4652-d0c6-4cd5-9d25-3b1aac267856",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = total_df['Q']\n",
    "swe = total_df['SWE']\n",
    "scaled = total_df['SWE_scaled']\n",
    "area = total_df['Area']\n",
    "station = total_df['GRDC_No']\n",
    "lat = total_df['Latitude']\n",
    "avg_slope = total_df['Avg Slope']\n",
    "max_slope = total_df['Max Slope']\n",
    "avg_aridity = total_df['Aridity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f05a1e4-d159-4c85-a20b-d551ac921a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_types = total_df.iloc[:, 11:-16]\n",
    "soil_types = total_df.iloc[:, -16:]\n",
    "\n",
    "# lc_types = lc_types/100\n",
    "# soil_types = soil_types/100\n",
    "\n",
    "# print(lc_types)\n",
    "# print(soil_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d4f7c4-1579-41ee-9e5d-f7d62ddb6b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04218477 0.18426304 0.10942487 0.05021058 0.49462185 0.04690124\n",
      " 0.0625     0.18181818]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "\n",
    "lc_in = lc_types.idxmax(axis=1)\n",
    "soil_in = soil_types.idxmax(axis=1)\n",
    "# print(lc_class)\n",
    "\n",
    "# lat_in = (lat*np.pi)/180\n",
    "# Convert q and swe to NumPy arrays\n",
    "scaled_in = scaled.to_numpy()  # Assume scaled is a pandas Series\n",
    "area_in = area.to_numpy()      # Assume area is a pandas Series\n",
    "lat_in = lat.to_numpy()\n",
    "avg_slope_in = avg_slope.to_numpy()\n",
    "# avg_slope_in = (avg_slope_in*np.pi)/180\n",
    "max_slope_in = max_slope.to_numpy()\n",
    "# max_slope_in = (max_slope_in*np.pi)/180\n",
    "avg_aridity_in = avg_aridity.to_numpy()\n",
    "y = q.to_numpy()            # Target variable\n",
    "\n",
    "# lc_in = lc_types.to_numpy()\n",
    "# soil_in = soil_types.to_numpy()\n",
    "\n",
    "# Convert q and swe to NumPy arrays\n",
    "# scaled_in = scaled_in/max(scaled_in)  # Assume scaled is a pandas Series\n",
    "# area_in = area_in/max(area_in)      # Assume area is a pandas Series\n",
    "# lat_in = lat_in/180\n",
    "# avg_slope_in = avg_slope_in/180\n",
    "# max_slope_in = max_slope_in/180\n",
    "# avg_aridity_in = avg_aridity_in/max(avg_aridity_in)\n",
    "# y = y/max(y)            # Target variable\n",
    "# lc_in = lc_types/max(lc_types)\n",
    "# soil_in = soil_types/max(soil_types)\n",
    "\n",
    "# Step 1: Replace zeros with a small value\n",
    "scaled_in[scaled_in == 0] = 1e-9\n",
    "area_in[area_in == 0] = 1e-9\n",
    "lat_in[lat_in == 0] = 1e-9\n",
    "avg_slope_in[avg_slope_in == 0] = 1e-9\n",
    "max_slope_in[max_slope_in == 0] = 1e-9\n",
    "avg_aridity_in[avg_aridity_in == 0] = 1e-9\n",
    "y[y == 0] = 1e-9\n",
    "\n",
    "# Step 2: Combine features into a 2D array\n",
    "X = np.column_stack((scaled_in, area_in, lat_in, avg_slope_in, max_slope_in, avg_aridity_in, lc_in, soil_in))  # Shape will be (num_samples, 2)\n",
    "# Step 3: Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "print(X_scaled[0])\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfad9bb9-8037-44d3-9dfe-935e17bc26a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.45853367 0.23928474 0.15831635]\n",
      "Top features for PC1:\n",
      "Latitude         0.952120\n",
      "Max Slope        0.234525\n",
      "Land Cover       0.118506\n",
      "Soil Texture     0.100598\n",
      "Avg Slope        0.098004\n",
      "Aridity Index    0.065594\n",
      "Area             0.018791\n",
      "SWE              0.006407\n",
      "Name: PC1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_df = total_df.drop(['Date','GRDC_No'],axis=1)\n",
    "# scaled_X = scaler.fit_transform(X_df)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_scaled)\n",
    "pca_X = pca.transform(X_scaled)\n",
    "\n",
    "# Create a new DataFrame with the principal components\n",
    "pca_df = pd.DataFrame(data=pca_X, columns=['pc1','pc2','pc3'])\n",
    "\n",
    "# Display the explained variance ratio\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "# To see the importance of each feature for the principal components\n",
    "# The components_ attribute holds the loadings (how much each feature contributes to each principal component)\n",
    "loadings = pd.DataFrame(pca.components_, columns=['SWE','Area','Latitude','Avg Slope','Max Slope','Aridity Index','Land Cover','Soil Texture'], index=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "# print(loadings)\n",
    "\n",
    "# Optionally, you can sort the loadings to identify the most important features\n",
    "# Example: Sort by PC1 loadings\n",
    "print(\"Top features for PC1:\")\n",
    "print(loadings.loc['PC1'].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61070ad2-b186-4331-b692-fa923f93dd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00309059 -0.00448671  0.00757038 ...  0.00041864  0.00438002\n",
      "  -0.03269477]\n",
      " [-0.00308995 -0.00448492  0.00756625 ...  0.0003789   0.00408544\n",
      "  -0.03451023]\n",
      " [-0.0030903  -0.00448592  0.00756856 ...  0.00040109  0.00424991\n",
      "  -0.03349661]\n",
      " ...\n",
      " [-0.0115581  -0.00242105 -0.00208091 ... -0.00273017 -0.00270393\n",
      "   0.00366328]\n",
      " [-0.0115581  -0.00242104 -0.00208093 ... -0.0027304  -0.00270566\n",
      "   0.00365257]\n",
      " [-0.0115581  -0.00242105 -0.00208091 ... -0.00273025 -0.00270451\n",
      "   0.00365971]]\n"
     ]
    }
   ],
   "source": [
    "U, D, VT = np.linalg.svd(X_scaled, full_matrices=False)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e52aed4d-2aee-4c97-8e91-65cb03727305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f031f01c-bf6d-4b59-bce1-8df12ebe7730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/home/users/arvalcarcel/.local/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-05-05 18:19:17.701248: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 41, but received input with shape (None, 7)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 7), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     31\u001b[0m     X_train, y_train,\n\u001b[1;32m     32\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,  \u001b[38;5;66;03m# High epochs with early stopping\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     34\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test),\n\u001b[1;32m     35\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     36\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, lr_scheduler]\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/src/layers/input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    224\u001b[0m             value,\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         }:\n\u001b[0;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 41, but received input with shape (None, 7)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 7), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(512, input_dim=41, activation='leaky_relu'),\n",
    "    Dense(256, activation='leaky_relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='leaky_relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='leaky_relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='leaky_relu'),\n",
    "    Dense(1, activation='linear')  # Linear activation for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "from tensorflow.keras.optimizers import RMSprop, Nadam\n",
    "from tensorflow.keras.losses import Huber\n",
    "model.compile(optimizer=Nadam(learning_rate=1e-3), loss=Huber(delta=0.1), metrics=['mae'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=150,  # High epochs with early stopping\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c5984-08c1-4c05-86df-c592ac5657ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test MAPE: {test_loss}\")\n",
    "\n",
    "# Step 8: Predict\n",
    "predictions = model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e592d-7271-4ca1-aa2b-e9cab850facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Predict values\n",
    "y_pred = model.predict(X_test)\n",
    "# y_pred = np.expm1(y_pred)\n",
    "\n",
    "# Step 2: Create the scatter plot\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test*max(y), y_pred*max(y), alpha=0.5, label=\"Predicted vs Actual\")\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='1:1 (y=x)', linewidth=2)\n",
    "\n",
    "# Step 3: Add labels, title, and legend\n",
    "plt.title(\"Streamflow Actual vs Predicted Values\")\n",
    "plt.xlabel(\"Actual Values [m${^3}$/s]\")\n",
    "plt.ylabel(\"Predicted Values [m${^3}$/s]\")\n",
    "plt.legend()\n",
    "# plt.xlim(0,25000)\n",
    "# plt.ylim(0,25000)\n",
    "plt.grid(alpha=0.5)\n",
    "\n",
    "# Step 4: Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc250d7-5e86-4f0a-a9b8-3080952c571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # zoomed in plot\n",
    "# y_pred = model.predict(X_test)\n",
    "# # y_pred = np.expm1(y_pred)\n",
    "\n",
    "# # Step 2: Create the scatter plot\n",
    "# plt.figure(figsize=(6,6))\n",
    "# plt.scatter(y_test*max(y), y_pred*max(y), alpha=0.5, label=\"Predicted vs Actual\")\n",
    "# plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='1:1 (y=x)', linewidth=2)\n",
    "\n",
    "# # Step 3: Add labels, title, and legend\n",
    "# plt.title(\"Streamflow Actual vs Predicted Values\")\n",
    "# plt.xlabel(\"Actual Values [m${^3}$/s]\")\n",
    "# plt.ylabel(\"Predicted Values [m${^3}$/s]\")\n",
    "# plt.legend()\n",
    "# plt.xlim(0,25000)\n",
    "# plt.ylim(0,25000)\n",
    "# plt.grid(alpha=0.5)\n",
    "\n",
    "# # Step 4: Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8bad4-e928-4660-a48f-faf43581a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nse(predictions, targets):\n",
    "    return 1 - (np.sum((targets - predictions) ** 2) / np.sum((targets - np.mean(targets)) ** 2))\n",
    "\n",
    "nse(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26464f2-d1f9-4048-a059-2107887ca957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and validation loss from the training history\n",
    "test_values = y_test*max(y)\n",
    "pred_values = y_pred*max(y)\n",
    "\n",
    "test_values = np.ravel(test_values)\n",
    "pred_values = np.ravel(pred_values)\n",
    "\n",
    "# Create a new DataFrame\n",
    "pred_df = pd.DataFrame({\n",
    "    'Test Values': test_values,\n",
    "    'Predicted Values': pred_values\n",
    "})\n",
    "\n",
    "file_path =  '/global/scratch/users/arvalcarcel/CSMUB/RESULTS/CNN/test_vs_pred_values.csv'\n",
    "\n",
    "# Export to a CSV file\n",
    "# pred_df.to_csv(file_path, index=False)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming test_values and pred_values are 1D arrays or lists\n",
    "r_squared = r2_score(test_values, pred_values)\n",
    "print(f\"R-squared value: {r_squared}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd983605-9c74-4214-842a-6553917f47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Compute MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Extract loss and validation loss from the training history\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "# Create a new DataFrame\n",
    "loss_df = pd.DataFrame({\n",
    "    'Epoch': range(1, len(training_loss) + 1),  # Adding epoch numbers\n",
    "    'Training Loss': training_loss,\n",
    "    'Validation Loss': validation_loss\n",
    "})\n",
    "\n",
    "file_path =  '/global/scratch/users/arvalcarcel/CSMUB/RESULTS/CNN/training_validation_losses.csv'\n",
    "\n",
    "# Export to a CSV file\n",
    "loss_df.to_csv(file_path, index=False)\n",
    "\n",
    "# print((max(y_train) - min(y_train))**2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss',color='r',alpha=0.7)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "# plt.ylim(0.00001, 0.0002)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a62da-76a0-40f2-8dde-20df4b07ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "residuals = y_test - model.predict(X_test)\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291530c-2b52-42dd-bb1f-7fc94e4f5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y_test and y_pred as a time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(len(y_test)), y_test*max(y), label=\"Actual Values (y_test)\", color=\"blue\", alpha=0.7)\n",
    "plt.plot(np.arange(len(y_pred)), y_pred*max(y), label=\"Predicted Values (y_pred)\", color=\"red\", alpha=0.7)\n",
    "plt.title(\"Actual vs Predicted Values of Test Dataset\")\n",
    "plt.xlabel(\"Test Dataset\")\n",
    "plt.ylabel(\"Streamflow [m${^3}$/s]\")\n",
    "plt.xlim(0,len(y_test))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf3555a-6f60-440e-852d-d53b7a29fb42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot y_test and y_pred as a time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot((np.arange(len(y_test)))[0:500], (y_test*max(y))[0:500], label=\"Actual Values (y_test)\", color=\"blue\", alpha=0.7)\n",
    "plt.plot((np.arange(len(y_pred)))[0:500], (y_pred*max(y))[0:500], label=\"Predicted Values (y_pred)\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Actual vs Predicted Values of Test Dataset (element 0 to 500)\")\n",
    "plt.xlabel(\"Test Dataset\")\n",
    "plt.ylabel(\"Streamflow [m${^3}$/s]\")\n",
    "# plt.xlim(0,len(y_test))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31313ea-9c5b-469f-93c1-c104b5df69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(scaled,q)\n",
    "plt.title('Raw Data - All Basins')\n",
    "plt.xlabel('Surface Water Extent Area [km${^2}$]')\n",
    "plt.ylabel('Streamflow [m${^3}$/s]')\n",
    "# plt.savefig('global/scratch/users/arvalcarcel/CSMUB/RESULTS/CNN/rawdata.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006963d-0708-4102-833b-7ed7596a5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(area,q,color='g')\n",
    "plt.title('Raw Data - All Basins')\n",
    "plt.xlabel('Total Basin Area [km${^2}$]')\n",
    "plt.ylabel('Streamflow [m${^3}$/s]')\n",
    "# plt.savefig('global/scratch/users/arvalcarcel/CSMUB/RESULTS/CNN/rawdata.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59885bec-2ec6-4b93-82de-a2585e2ce843",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lat,q,color='r')\n",
    "plt.title('Raw Data - All Basins')\n",
    "plt.xlabel('Latitude [deg]')\n",
    "plt.ylabel('Streamflow [m${^3}$/s]')\n",
    "# plt.savefig('global/scratch/users/arvalcarcel/CSMUB/RESULTS/CNN/rawdata.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017459b5-e7da-49d5-adc9-832a261d3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(slope,q,color='purple')\n",
    "plt.title('Raw Data - All Basins')\n",
    "plt.xlabel('Slope [deg]')\n",
    "plt.ylabel('Streamflow [m${^3}$/s]')\n",
    "# plt.savefig('global/scratch/users/arvalcarcel/CSMUB/RESULTS/CNN/rawdata.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589bbdf-bbe6-414a-81fa-12e47bf8dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lc_class = lc_types.idxmax(axis=1)\n",
    "plt.scatter(lc_class,q,color='orange')\n",
    "plt.title('Raw Data - All Basins')\n",
    "plt.xlabel('Land Cover Type')\n",
    "plt.ylabel('Streamflow [m${^3}$/s]')\n",
    "# plt.savefig('global/scratch/users/arvalcarcel/CSMUB/RESULTS/CNN/rawdata.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade19ef-75d4-4940-b49f-c0c775061bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soil_class = soil_types.idxmax(axis=1)\n",
    "\n",
    "plt.scatter(soil_class,q,color='darkorange')\n",
    "plt.title('Raw Data - All Basins')\n",
    "plt.xlabel('Soil Texture')\n",
    "plt.ylabel('Streamflow [m${^3}$/s]')\n",
    "# plt.savefig('global/scratch/users/arvalcarcel/CSMUB/RESULTS/CNN/rawdata.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5644d7-eb60-4f52-bf39-667ae642a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = total_df.groupby('Area')['Q'].mean()\n",
    "\n",
    "# Extract unique areas and their corresponding average Q values\n",
    "area_unique = grouped.index\n",
    "q_avg = grouped.values\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(area_unique, q_avg, color='g')\n",
    "plt.xlabel('Total Basin Area')\n",
    "plt.ylabel('Average Streamflow')\n",
    "plt.title('Average Streamflow vs Total Basin Area')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b731448-0d5e-4deb-b732-0e7eef979259",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = total_df.groupby('Latitude')['Q'].mean()\n",
    "\n",
    "# Extract unique areas and their corresponding average Q values\n",
    "lat_unique = grouped.index\n",
    "q_avg = grouped.values\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lat_unique, q_avg,color='red')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Average Streamflow')\n",
    "plt.title('Average Streamflow vs Basin Latitude')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c2122-8f97-4ca9-a2ca-61a3b6588fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/global/scratch/users/arvalcarcel/CSMUB/RESULTS/CNN/rnn_plot.png'\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Plot and save the model\n",
    "plot_model(model, to_file=filename, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
